{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process DeepSea datset\n",
    "\n",
    "In this notebook, the DeepSea dataset is acquired and parsed to generate a smaller transcription factor dataset, consisting of CTCF, GABP, SP1, SRF, and YY1, for K562 and HepG2 celltypes. The dataset is first downloaded directly from DeepSea webserver and then custom scripts convert these into a h5py file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Bla Bla Bla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, h5py, scipy.io\n",
    "import numpy as np\n",
    "import subprocess as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# download DeepSea dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading DeepSea dataset\n",
      "decompressing DeepSea dataset\n"
     ]
    }
   ],
   "source": [
    "# download deepsea dataset into data folder, if it does not exist\n",
    "if not os.path.isdir('../data/deepsea_train'):\n",
    "    print('downloading DeepSea dataset')\n",
    "    os.system('wget http://deepsea.princeton.edu/media/code/deepsea_train_bundle.v0.9.tar.gz -O ../data/deepsea_train_bundle.v0.9.tar.gz') \n",
    "    print('decompressing DeepSea dataset')\n",
    "    os.system('tar xzvf ../data/deepsea_train_bundle.v0.9.tar.gz -C ../data ')\n",
    "    os.system('rm ../data/deepsea_train_bundle.v0.9.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_DeepSea_subset(filepath, class_range=range(918)):\n",
    "    \"\"\" function to load DeepSea's dataset of specific transcription factors specified \n",
    "        by class_range. The output is a h5py file with the sequences represented\n",
    "        as a 4D tensor for input into Lasagne/Theano convolution layers.  The labels\n",
    "        is a 2D matrix where each row corresponds to a new sequence. \"\"\"\n",
    "    \n",
    "    def data_subset(y, class_range):\n",
    "        \" gets a subset of data in the class_range\"\n",
    "        data_index = []\n",
    "        for i in class_range:\n",
    "            index = np.where(y[:, i] == 1)[0]\n",
    "            data_index = np.concatenate((data_index, index), axis=0)\n",
    "        unique_index = np.unique(data_index)\n",
    "        return unique_index.astype(int)\n",
    "\n",
    "    print(\"loading training data\")\n",
    "    trainmat = h5py.File(os.path.join(filepath,'train.mat'), 'r')\n",
    "    y_train = np.transpose(trainmat['traindata'], axes=(1,0))\n",
    "    index = data_subset(y_train, class_range)\n",
    "    y_train = y_train[:,class_range]\n",
    "    y_train = y_train[index,:]\n",
    "    X_train = np.transpose(trainmat['trainxdata'], axes=(2,1,0)) \n",
    "    X_train = X_train[index,:,:]\n",
    "    X_train = X_train[:,[0,2,1,3],:]\n",
    "    X_train = np.expand_dims(X_train, axis=3)\n",
    "    train = (X_train.astype(np.int8), y_train.astype(np.int8))\n",
    "    \n",
    "    print(\"loading validation data\")\n",
    "    validmat = scipy.io.loadmat(os.path.join(filepath,'valid.mat'))\n",
    "    y_valid = np.array(validmat['validdata'])\n",
    "    index = data_subset(y_valid,class_range)\n",
    "    y_valid = y_valid[:, class_range]\n",
    "    y_valid = y_valid[index,:]\n",
    "    X_valid = np.transpose(validmat['validxdata'], axes=(0,1,2))  \n",
    "    X_valid = X_valid[index,:,:]\n",
    "    X_valid = X_valid[:,[0,2,1,3],:]\n",
    "    X_valid = np.expand_dims(X_valid, axis=3)\n",
    "    valid = (X_valid.astype(np.int8), y_valid.astype(np.int8))\n",
    "    \n",
    "    print(\"loading test data\")\n",
    "    testmat = scipy.io.loadmat(os.path.join(filepath,'test.mat'))\n",
    "    y_test = np.array(testmat['testdata'])\n",
    "    index = data_subset(y_test,class_range)\n",
    "    y_test = y_test[:, class_range]\n",
    "    y_test = y_test[index,:]\n",
    "    X_test = np.transpose(testmat['testxdata'], axes=(0,1,2)) \n",
    "    X_test = X_test[index,:,:]\n",
    "    X_test = X_test[:,[0,2,1,3],:]\n",
    "    X_test = np.expand_dims(X_test, axis=3)\n",
    "    test = (X_test.astype(np.int8), y_test.astype(np.int8))\n",
    "\n",
    "    return train, valid, test \n",
    "\n",
    "def process_DeepSea_subset(train, valid, valid_percentage=0.1):\n",
    "    \"\"\"merge training and validation data, shuffle, and reallocate \n",
    "       based on 90% training and 10% cross-validation \"\"\"\n",
    "    \n",
    "    X_train = np.vstack([train[0], valid[0]])\n",
    "    Y_train = np.vstack([train[1], valid[1]])\n",
    "    index = np.random.permutation(X_train.shape[0])\n",
    "    cutoff = np.round(X_train.shape[0]*valid_percentage).astype(int)\n",
    "    valid = (X_train[:cutoff], Y_train[:cutoff])\n",
    "    train = (X_train[cutoff:], Y_train[cutoff:])\n",
    "    \n",
    "    return train, valid\n",
    "\n",
    "\n",
    "def save_DeepSea_subset(grp, train, valid, test):\n",
    "    \"\"\" save to h5py dataset \"\"\"\n",
    "    print(\"saving datset\")\n",
    "    X_train = grp.create_dataset('X_train', data=train[0], dtype='int8', compression=\"gzip\")\n",
    "    Y_train = grp.create_dataset('Y_train', data=train[1], dtype='int8', compression=\"gzip\")\n",
    "    X_valid = grp.create_dataset('X_valid', data=valid[0], dtype='int8', compression=\"gzip\")\n",
    "    Y_valid = grp.create_dataset('Y_valid', data=valid[1], dtype='int8', compression=\"gzip\")\n",
    "    X_test = grp.create_dataset('X_test', data=test[0], dtype='int8', compression=\"gzip\")\n",
    "    Y_test = grp.create_dataset('Y_test', data=test[1], dtype='int8', compression=\"gzip\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parse subset of DeepSea dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training data\n",
      "loading validation data\n",
      "loading test data\n"
     ]
    }
   ],
   "source": [
    "core_names = ['Arid3a', 'CEBPB', 'FOSL1', 'Gabpa', 'MAFK', 'MAX', \n",
    "              'MEF2A', 'NFYB', 'SP1', 'SRF', 'STAT1', 'YY1']\n",
    "core_index = [592, 602, 344, 345, 635, 636, 349, 642, 359, 361, 661, 369]\n",
    "#core_index =  [547, 602, 344, 345, 635, 636, 218, 642, 237, 238, 535, 369]\n",
    "\n",
    "# save datasets in a hdf5 file under groups HepG2 and K562\n",
    "data_path = '../data/deepsea_train/'\n",
    "\n",
    "# load deep sea dataset\n",
    "train, valid, test = load_DeepSea_subset(data_path, class_range=core_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training samples for each class\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([27652, 85354, 19724, 34194, 43528, 87290,  9792, 22758, 17450,\n",
       "        7528,  4516, 31146])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"number of training samples for each class\")\n",
    "np.sum(train[1], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, valid = process_DeepSea_subset(train, valid, valid_percentage=0.1)        \n",
    "with h5py.File('../data/invivo_dataset.h5', 'w') as fout:\n",
    "    X_train = fout.create_dataset('X_train', data=train[0], dtype='int8', compression=\"gzip\")\n",
    "    Y_train = fout.create_dataset('Y_train', data=train[1], dtype='int8', compression=\"gzip\")\n",
    "    X_valid = fout.create_dataset('X_valid', data=valid[0], dtype='int8', compression=\"gzip\")\n",
    "    Y_valid = fout.create_dataset('Y_valid', data=valid[1], dtype='int8', compression=\"gzip\")\n",
    "    X_test = fout.create_dataset('X_test', data=test[0], dtype='int8', compression=\"gzip\")\n",
    "    Y_test = fout.create_dataset('Y_test', data=test[1], dtype='int8', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
